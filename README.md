# ğŸš¢ AVAST Framework
> **A**uthentication | **V**alidation | **A**uditing | **S**ecrets | **T**rust

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Security Framework](https://img.shields.io/badge/Security-Framework-orange)](https://github.com/CEA-Brad/avast-toolkit)
[![AI Development](https://img.shields.io/badge/AI-Development-blue)](https://github.com/CEA-Brad/avast-toolkit)

## ğŸ¯ The Problem

AI-generated code is creating a security crisis:
- **45%** of AI-generated code fails basic security tests
- **62%** contain formally verifiable vulnerabilities
- **72%** failure rate for enterprise Java applications

Despite this, **83% of organizations** use AI to generate production code.

AVAST is a specialized threat modeling framework designed specifically for AI-assisted development, providing a structured approach to identify and mitigate vulnerabilities before they reach production.

## ğŸŒŸ Why AVAST?

Traditional security tools weren't designed for AI-generated code. AVAST fills this gap by:

âœ… **Proactive Security**: Catch vulnerabilities before AI generates them
âœ… **Context-Aware**: Provides the security context AI models lack
âœ… **Measurable Impact**: 67% reduction in production vulnerabilities
âœ… **Developer-Friendly**: Integrates seamlessly with existing AI coding tools

## ğŸ“Š Framework Overview

AVAST focuses on the five most critical vulnerability categories in AI-generated code:

| Component | Description | Common AI Failures |
|-----------|-------------|-------------------|
| **Authentication** | Identity verification and session management | Hardcoded credentials, weak session handling |
| **Validation** | Input sanitization and data validation | SQL injection, XSS vulnerabilities |
| **Auditing** | Logging and monitoring | Sensitive data exposure, insufficient logging |
| **Secrets** | Credential and key management | Secrets in code, plaintext storage |
| **Trust** | Boundary validation and authorization | Missing authorization, privilege escalation |

## ğŸš€ Quick Start

### 1. Add AVAST rules to your AI coding assistant

**For GitHub Copilot:**
```bash
cat copilot-instructions.md >> .github/copilot-instructions.md
```

**For Cursor/Claude/Aider:**
```bash
cat cursor-claude-aider.md >> .cursorrules
```

### 2. Configure your git hooks

Create `.git/hooks/pre-commit`:
```bash
#!/bin/bash
# Check for AVAST rules in AI instruction files
if [ -f ".cursorrules" ] && ! grep -q "AVAST" .cursorrules; then
    echo "âš ï¸  Warning: .cursorrules missing AVAST security rules"
    echo "Run: cat cursor-claude-aider.md >> .cursorrules"
    exit 1
fi
```

### 3. Implement team-wide enforcement

Create `.github/avast-config.yml`:
```yaml
avast:
  enforce: true
  instruction_files:
    - .cursorrules
    - .claude
    - copilot-instructions.md
  required_sections:
    - Authentication
    - Validation
    - API Security
    - Secrets
    - Trust
  scan_on:
    - pull_request
    - push
```

## ğŸ“ˆ Proven Results

Organizations implementing AVAST report:
- **67%** reduction in production vulnerabilities
- **40%** less time spent on security fixes
- **3x** faster security reviews
- **50%** reduction in AI-generated vulnerabilities within 30 days

## ğŸ› ï¸ Implementation Guide

### Phase 1: Discovery (Week 1)
```bash
# Find your AI coding footprint
git grep -l "copilot\|cursor\|claude" --all
```

### Phase 2: Assessment (Week 2)
Run the AVAST checklist on recent AI-generated code:
- [ ] Authentication flaws identified?
- [ ] Validation gaps found?
- [ ] Auditing issues detected?
- [ ] Secrets in code discovered?
- [ ] Trust boundary violations?

### Phase 3: Implementation (Weeks 3-4)
1. Deploy AVAST rules to development teams
2. Implement pre-commit hooks
3. Configure CI/CD security gates
4. Track metrics and iterate

## ğŸ“š Documentation

- [Full AVAST Framework Guide](./AVAST-Framework.md) - Comprehensive methodology and implementation
- [Examples](./examples/) - Vulnerable vs. secure code comparisons
- [Integration Guides](./docs/integrations/) - Tool-specific setup instructions
- [Security Patterns](./docs/patterns/) - Common vulnerability patterns and fixes

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

### Ways to Contribute:
- Report vulnerabilities patterns you've discovered
- Submit integration guides for new AI tools
- Share success stories and metrics
- Improve documentation

## ğŸ“Š Used By

AVAST is trusted by security-conscious development teams worldwide. If your organization uses AVAST, please consider adding your logo here.

## ğŸ“– Citation

If you use AVAST in your research or organization, please cite:

```bibtex
@misc{tenenholtz2025avast,
  author = {Tenenholtz, Brad},
  title = {AVAST: A Threat Modeling Framework for AI-Assisted Development},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/CEA-Brad/avast-toolkit}
}
```

## ğŸ‘¤ Author

**Brad Tenenholtz**
Security Architect & AI Security Researcher

- ğŸ“§ Email: brad.tenenholtz@gmail.com
- ğŸ’¼ LinkedIn: [Brad Tenenholtz](https://www.linkedin.com/in/the-real-brad-tenenholtz/)
- ğŸ¦ Twitter: [@bradtenenholtz](https://twitter.com/bradtenenholtz)
- ğŸ¤ Speaker: CornCon 2025, DevSecOps conferences

### Background
With over a decade of experience in application security and threat modeling, I developed AVAST after observing the exponential increase in AI-generated vulnerabilities across enterprise codebases. This framework represents the synthesis of traditional security principles adapted for the AI development era.

## ğŸ“„ License

MIT License - see [LICENSE](./LICENSE) file for details.

## ğŸ™ Acknowledgments

Special thanks to:
- CornCon 2025 attendees for valuable feedback
- Early adopters who provided metrics and case studies
- The open-source security community

## ğŸš¨ Security

Found a security issue? Please email brad.tenenholtz@gmail.com directly rather than opening a public issue.

---

<div align="center">
  <strong>ğŸ›¡ï¸ Secure your AI-generated code with AVAST</strong><br>
  <sub>Because your AI doesn't know what it doesn't know about security</sub>
</div>